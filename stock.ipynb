{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def configure_jupyter():\n",
    "    jupyter_config_dir = os.path.expanduser('~/.jupyter')\n",
    "    if not os.path.exists(jupyter_config_dir):\n",
    "        os.makedirs(jupyter_config_dir)\n",
    "    \n",
    "    config_path = os.path.join(jupyter_config_dir, 'jupyter_notebook_config.json')\n",
    "    \n",
    "    config = {\n",
    "        \"NotebookApp\": {\n",
    "            \"clear_output_before_save\": True  # Fixed: true -> True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "# Configure Jupyter\n",
    "configure_jupyter()\n",
    "\n",
    "def clean_checkpoints():\n",
    "    checkpoint_dir = '.ipynb_checkpoints'\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        shutil.rmtree(checkpoint_dir)\n",
    "        print(f\"Removed {checkpoint_dir}\")\n",
    "\n",
    "# Clean up checkpoints\n",
    "clean_checkpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "import talib as ta  # Using TA-Lib instead of pandas_ta\n",
    "\n",
    "# Set device for PyTorch\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def download_stock_data(ticker, start_date, end_date):\n",
    "    # Download stock data using yfinance\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data\n",
    "\n",
    "def get_high_interest_tickers(min_volume=1000000, min_option_interest=1000, max_stocks=20):\n",
    "    \"\"\"\n",
    "    Find stocks with high trading volume and significant options interest, focusing on mid-cap stocks with higher volatility.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Stock Screening Started ===\")\n",
    "    \n",
    "    # High-growth Tech and Software From CHATGPT\n",
    "    # Looked for stocks that can swing 30%+ based on news and have high volatility\n",
    "    stocks_picks_from_chatgpt = [\n",
    "    \"GME\",   # GameStop - Meme stock with high short interest; massive swings\n",
    "    \"AMC\",   # AMC Entertainment - Another meme stock, highly volatile\n",
    "    \"BBBY\",  # Bed Bath & Beyond - Recent volatility and high short interest\n",
    "    \"PLUG\",  # Plug Power - Volatile in renewable energy space\n",
    "    \"OCGN\",  # Ocugen - Biotech with huge price swings around news\n",
    "    \"RIOT\",  # Riot Blockchain - Crypto stock with large fluctuations\n",
    "    \"MARA\",  # Marathon Digital - Another crypto-related stock with high volatility\n",
    "    \"SPCE\",  # Virgin Galactic - Space exploration, speculative with swings\n",
    "    \"TLRY\",  # Tilray - Cannabis stock, often swings 30%+ based on news\n",
    "    \"NOK\",   # Nokia - Telecom stock with periods of high volatility\n",
    "    \"SNDL\",  # Sundial Growers - Cannabis stock with massive swings\n",
    "    \"XPEV\",  # XPeng - EV stock with rapid growth and volatile price movements\n",
    "    \"LI\",    # Li Auto - Chinese EV maker, often sees large price fluctuations\n",
    "    \"ENPH\",  # Enphase Energy - Renewable energy with volatile price changes\n",
    "    \"ZM\",    # Zoom Video - Still volatile post-pandemic with earnings impact\n",
    "    \"FUBO\",  # fuboTV - Streaming company with high volatility\n",
    "    \"SKLZ\",  # Skillz - Gaming stock that moves with hype and news\n",
    "    \"CLOV\",  # Clover Health - Healthcare stock with frequent big swings\n",
    "    \"PLTR\",  # Palantir - Tech stock, volatile on news and earnings\n",
    "    \"RKT\"    # Rocket Companies - Mortgage sector, moves with interest rate changes\n",
    "]\n",
    "\n",
    "    \n",
    "    # Combine all sectors and remove duplicates\n",
    "    all_tickers = list(set(\n",
    "        stocks_picks_from_chatgpt\n",
    "    ))\n",
    "    \n",
    "    print(f\"Analyzing {len(all_tickers)} growth and momentum stocks\")\n",
    "    print(f\"Minimum daily volume: {min_volume:,}\")\n",
    "    print(f\"Minimum option interest: {min_option_interest:,}\")\n",
    "    \n",
    "    print(f\"Found {len(all_tickers)} unique tickers to analyze\")\n",
    "    print(f\"Minimum daily volume: {min_volume:,}\")\n",
    "    print(f\"Minimum option interest: {min_option_interest:,}\")\n",
    "\n",
    "    high_interest_stocks = []\n",
    "    stocks_processed = 0\n",
    "    errors = 0\n",
    "    \n",
    "    for i, ticker in enumerate(all_tickers, 1):\n",
    "        try:\n",
    "            print(f\"Processing {ticker}...\")\n",
    "            stock = yf.Ticker(ticker)\n",
    "            info = stock.info\n",
    "            \n",
    "            # Get options expiration dates\n",
    "            try:\n",
    "                expirations = stock.options\n",
    "                if not expirations:\n",
    "                    print(f\"No options data available for {ticker}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Get the first expiration date\n",
    "                first_expiration = expirations[0]\n",
    "                options = stock.option_chain(first_expiration)\n",
    "                total_oi = options.calls['openInterest'].sum() + options.puts['openInterest'].sum()\n",
    "                put_call_ratio = options.puts['openInterest'].sum() / max(1, options.calls['openInterest'].sum())\n",
    "                \n",
    "                print(f\"{ticker} options expiration: {first_expiration}, Total OI: {total_oi}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error getting options data for {ticker}: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "            # Check volume and options interest\n",
    "            volume = info.get('averageVolume', 0)\n",
    "            if volume > min_volume and total_oi > min_option_interest:\n",
    "                high_interest_stocks.append({\n",
    "                    'ticker': ticker,\n",
    "                    'name': info.get('shortName', 'N/A'),\n",
    "                    'price': info.get('currentPrice', 0),\n",
    "                    'volume': volume,\n",
    "                    'options_oi': total_oi,\n",
    "                    'put_call_ratio': put_call_ratio,\n",
    "                    'expiration': first_expiration,  # Added expiration date\n",
    "                    'market_cap': info.get('marketCap', 0),\n",
    "                    'beta': info.get('beta', 0)\n",
    "                })\n",
    "                print(f\"Added {ticker} to high interest stocks\")\n",
    "            \n",
    "            stocks_processed += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            print(f\"Error processing {ticker}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Sort by options open interest\n",
    "    high_interest_stocks.sort(key=lambda x: x['options_oi'], reverse=True)\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    df = pd.DataFrame(high_interest_stocks)\n",
    "    if not df.empty:\n",
    "        # Format the columns for display\n",
    "        df['market_cap'] = df['market_cap'].apply(lambda x: f\"${x/1e9:.1f}B\")\n",
    "        df['volume'] = df['volume'].apply(lambda x: f\"{x/1e6:.1f}M\")\n",
    "        df['options_oi'] = df['options_oi'].apply(lambda x: f\"{x/1000:.1f}K\")\n",
    "        df['put_call_ratio'] = df['put_call_ratio'].apply(lambda x: f\"{x:.2f}\")\n",
    "        df['price'] = df['price'].apply(lambda x: f\"${x:.2f}\")\n",
    "        df['beta'] = df['beta'].apply(lambda x: f\"{x:.2f}\")\n",
    "        \n",
    "        print(\"\\n=== Results ===\")\n",
    "        print(f\"Total stocks scanned: {stocks_processed}\")\n",
    "        print(f\"Stocks meeting criteria: {len(high_interest_stocks)}\")\n",
    "        print(f\"Errors encountered: {errors}\")\n",
    "        print(\"\\nTop Stocks by Options Interest:\")\n",
    "        print(df.head(max_stocks).to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\nNo stocks met the criteria. Try adjusting the minimum requirements.\")\n",
    "    \n",
    "    return [stock['ticker'] for stock in high_interest_stocks[:max_stocks]]\n",
    "\n",
    "def add_option_indicators(stock_data, ticker):\n",
    "    # Get options data\n",
    "    stock = yf.Ticker(ticker)\n",
    "    try:\n",
    "        opt = stock.option_chain('nearest')\n",
    "        has_options = True\n",
    "    except:\n",
    "        has_options = False\n",
    "    \n",
    "    # Basic price momentum indicators\n",
    "    df = stock_data.copy()\n",
    "    \n",
    "    # Weekly and Monthly Returns\n",
    "    df['1w_return'] = df['Close'].pct_change(periods=5)\n",
    "    df['2w_return'] = df['Close'].pct_change(periods=10)\n",
    "    df['1m_return'] = df['Close'].pct_change(periods=21)\n",
    "    \n",
    "    # Volatility\n",
    "    df['20d_vol'] = df['Close'].rolling(window=20).std()\n",
    "    \n",
    "    # Options-related indicators (if available)\n",
    "    if has_options:\n",
    "        df['call_oi'] = opt.calls['openInterest'].sum()\n",
    "        df['put_call_ratio'] = opt.puts['openInterest'].sum() / opt.calls['openInterest'].sum()\n",
    "        df['call_volume'] = opt.calls['volume'].sum()\n",
    "    \n",
    "    # Technical indicators using TA-Lib\n",
    "    close_prices = df['Close'].to_numpy().flatten()  # Ensure it is 1D\n",
    "\n",
    "    # Add StochRSI (returns fastk and fastd)\n",
    "    fastk, fastd = ta.STOCHRSI(close_prices, timeperiod=14)\n",
    "    df['STOCH_RSI_K'] = fastk\n",
    "    df['STOCH_RSI_D'] = fastd\n",
    "    \n",
    "    # Existing indicators\n",
    "    df['RSI'] = ta.RSI(close_prices, timeperiod=14)\n",
    "    df['MACD'], df['MACD_signal'], df['MACD_hist'] = ta.MACD(close_prices, fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    df['BBL'], df['BBM'], df['BBU'] = ta.BBANDS(close_prices, timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
    "    \n",
    "    # Momentum and Trend\n",
    "    df['above_200ma'] = df['Close'] > df['Close'].rolling(window=200).mean()\n",
    "    df['above_50ma'] = df['Close'] > df['Close'].rolling(window=50).mean()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    best_model_state = None  # Store best model state in memory\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            if not torch.isnan(loss):  # Skip if loss is NaN\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                output = model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()  # Save in memory\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            if best_model_state is not None:\n",
    "                model.load_state_dict(best_model_state)  # Load from memory\n",
    "            break\n",
    "            \n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch {epoch}: Train Loss = {avg_train_loss:.5f}, Val Loss = {avg_val_loss:.5f}')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def create_sliding_windows(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(data)):\n",
    "        current_price = data[i, 3]  # Close price\n",
    "        prev_price = data[i-1, 3]   # Previous close price\n",
    "        if prev_price != 0:  # Prevent division by zero\n",
    "            pct_change = (current_price - prev_price) / prev_price\n",
    "            X.append(data[i-window_size:i, :])  # Only append window once\n",
    "            y.append(pct_change)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def predict(model, features_scaled, window_size, ticker, scaler, stock_data, enhanced_data, num_weeks=4):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    last_window = features_scaled[-window_size:]\n",
    "    last_price = float(stock_data['Close'].iloc[-1])\n",
    "    last_features = enhanced_data.iloc[-1]\n",
    "    \n",
    "    # Start from today\n",
    "    today = datetime.now()\n",
    "    # Calculate days until next Friday (4 is Friday)\n",
    "    days_until_friday = (4 - today.weekday()) % 7\n",
    "    if days_until_friday == 0 and today.hour >= 16:  # If it's Friday after market close\n",
    "        days_until_friday = 7\n",
    "    \n",
    "    next_friday = today + timedelta(days=days_until_friday)\n",
    "    # Generate the next 4 Fridays\n",
    "    prediction_dates = [next_friday + timedelta(weeks=i) for i in range(num_weeks)]\n",
    "    \n",
    "    # Verify the dates are correct\n",
    "    print(\"\\nPredicting for these Fridays:\")\n",
    "    for date in prediction_dates:\n",
    "        print(date.strftime('%Y-%m-%d'))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        current_window = torch.FloatTensor(last_window).unsqueeze(0).to(device)\n",
    "        current_price = last_price\n",
    "        \n",
    "        for i in range(len(prediction_dates)):\n",
    "            # Predict percentage change\n",
    "            pct_change = model(current_window).item()\n",
    "            predicted_price = current_price * (1 + pct_change)\n",
    "            predictions.append(predicted_price)\n",
    "            \n",
    "            # Update for next prediction\n",
    "            current_price = predicted_price\n",
    "            \n",
    "            # Create new window by shifting\n",
    "            new_window = current_window.clone()\n",
    "            new_window[0, :-1, :] = new_window[0, 1:, :]\n",
    "            \n",
    "            # Update the last row with the new price\n",
    "            new_features = last_features.copy()\n",
    "            new_features['Close'] = predicted_price\n",
    "            new_features['Open'] = predicted_price\n",
    "            new_features['High'] = predicted_price\n",
    "            new_features['Low'] = predicted_price\n",
    "            \n",
    "            # Scale the new features\n",
    "            scaled_new_features = scaler.transform(new_features.values.reshape(1, -1))[0]\n",
    "            new_window[0, -1, :] = torch.FloatTensor(scaled_new_features).to(device)\n",
    "            current_window = new_window\n",
    "\n",
    "    # Create DataFrame for predicted prices\n",
    "    prediction_df = pd.DataFrame({\n",
    "        'Date': prediction_dates,\n",
    "        'Predicted Price': predictions,\n",
    "        'Pct_Change': [(p/last_price - 1)*100 for p in predictions]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nPredictions for upcoming Fridays:\")\n",
    "    print(prediction_df.to_string(float_format=lambda x: '{:.2f}'.format(x)))\n",
    "    \n",
    "    # Plot the predictions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(stock_data.index[-30:], stock_data['Close'][-30:], label='Historical')\n",
    "    plt.plot(prediction_df['Date'], prediction_df['Predicted Price'], label='Predicted', linestyle='--')\n",
    "    plt.title(f'{ticker} Stock Price Prediction (Weekly Friday Closes)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return prediction_df\n",
    "\n",
    "class StockLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(StockLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Deeper network\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_size * 2, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0_1 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0_1 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm1(x, (h0_1, c0_1))\n",
    "        out = self.layer_norm(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        h0_2 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0_2 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm2(out, (h0_2, c0_2))\n",
    "        out = self.layer_norm(out[:, -1, :])\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(ticker):\n",
    "    # Set date range\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=365*3)  # 3 years of data\n",
    "    \n",
    "    try:\n",
    "        # Download and prepare data\n",
    "        print(f\"Downloading data for {ticker} from {start_date} to {end_date}\")\n",
    "        stock_data = download_stock_data(ticker, start_date, end_date)\n",
    "        if stock_data.empty:\n",
    "            raise ValueError(f\"No data downloaded for {ticker}\")\n",
    "        \n",
    "        print(f\"Downloaded {len(stock_data)} days of data\")\n",
    "        \n",
    "        # Handle missing values first\n",
    "        stock_data = stock_data.ffill().bfill()\n",
    "        enhanced_data = add_option_indicators(stock_data, ticker)\n",
    "        \n",
    "        # Define available features\n",
    "        available_features = [\n",
    "            'Open', 'High', 'Low', 'Close', 'Volume',  # OHLCV\n",
    "            '1w_return', '2w_return', '1m_return', '20d_vol',  # Momentum\n",
    "            'RSI', 'MACD', 'MACD_signal', 'MACD_hist',  # Technical\n",
    "            'BBL', 'BBM', 'BBU',  # Bollinger Bands\n",
    "            'STOCH_RSI_K', 'STOCH_RSI_D',  # StochRSI\n",
    "            'above_200ma', 'above_50ma'  # Trend\n",
    "        ]\n",
    "        \n",
    "        # Handle missing values\n",
    "        enhanced_data = enhanced_data.ffill().bfill()\n",
    "        enhanced_data = enhanced_data.dropna()\n",
    "        \n",
    "        if len(enhanced_data) == 0:\n",
    "            raise ValueError(\"No valid data after preprocessing\")\n",
    "        \n",
    "        # Select features and scale\n",
    "        features = enhanced_data[available_features].values\n",
    "        scaler = MinMaxScaler()\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "        \n",
    "        # Create sliding windows\n",
    "        window_size = 20\n",
    "        X, y = create_sliding_windows(features_scaled, window_size)\n",
    "        \n",
    "        if len(X) == 0 or len(y) == 0:\n",
    "            raise ValueError(\"No valid sequences created\")\n",
    "        \n",
    "        # Split data\n",
    "        split_idx = int(len(X) * 0.8)\n",
    "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "        \n",
    "        # Add validation set\n",
    "        val_size = int(len(X_train) * 0.2)\n",
    "        X_val = X_train[-val_size:]\n",
    "        y_val = y_train[-val_size:]\n",
    "        X_train = X_train[:-val_size]\n",
    "        y_train = y_train[:-val_size]\n",
    "        \n",
    "        # Model parameters\n",
    "        input_size = len(available_features)\n",
    "        hidden_size = 128\n",
    "        num_layers = 2\n",
    "        output_size = 1\n",
    "        \n",
    "        # Initialize model\n",
    "        model = StockLSTM(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "        \n",
    "        # Training parameters\n",
    "        learning_rate = 0.0001\n",
    "        num_epochs = 50\n",
    "        batch_size = 32\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(torch.FloatTensor(X_train).to(device), \n",
    "                                    torch.FloatTensor(y_train).to(device))\n",
    "        val_dataset = TensorDataset(torch.FloatTensor(X_val).to(device), \n",
    "                                  torch.FloatTensor(y_val).to(device))\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        \n",
    "        # Optimizer with L2 regularization\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Train with validation\n",
    "        train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n",
    "        \n",
    "        return model, features_scaled, window_size, ticker, scaler, stock_data, enhanced_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in training: {str(e)}\")\n",
    "        return None, None, None, None, None, None, None\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "base_tickers = get_high_interest_tickers(\n",
    "    min_volume=500000,        # 1M daily volume\n",
    "    min_option_interest=500,  # 1K option interest\n",
    "    max_stocks=10             # Return top 10\n",
    ")\n",
    "\n",
    "# Add any additional tickers you want to analyze\n",
    "additional_tickers = [\"QS\", \"NIO\"]\n",
    "\n",
    "# Combine and remove duplicates\n",
    "tickers = list(set(base_tickers + additional_tickers))\n",
    "\n",
    "print(\"\\nAnalyzing these tickers:\", tickers)\n",
    "\n",
    "# Run the model for each ticker\n",
    "for ticker in tickers:\n",
    "    print(f\"Training model for {ticker}\")\n",
    "    result = train(ticker)\n",
    "    if result[0] is not None:\n",
    "        model, features_scaled, window_size, ticker, scaler, stock_data, enhanced_data = result\n",
    "        print(f\"\\nPredicting for {ticker}\")\n",
    "        predictions = predict(model, features_scaled, window_size, ticker, scaler, stock_data, enhanced_data, num_weeks=4)\n",
    "        \n",
    "        last_known_price = float(stock_data['Close'].iloc[-1])\n",
    "        last_date = stock_data.index[-1]\n",
    "        \n",
    "        print(f\"\\nLast Known Price (as of {last_date.strftime('%Y-%m-%d')}): ${last_known_price:.2f}\")\n",
    "        print(\"\\nPredictions for upcoming Fridays:\")\n",
    "        print(\"-\" * 60)\n",
    "        for i in range(4):\n",
    "            friday_price = float(predictions['Predicted Price'].iloc[i])\n",
    "            friday_date = predictions['Date'].iloc[i]\n",
    "            pct_change = ((friday_price/last_known_price - 1) * 100)\n",
    "            print(f\"Friday {friday_date.strftime('%Y-%m-%d')}: ${friday_price:.2f} (Change: {pct_change:+.2f}%)\")\n",
    "        print(\"-\" * 60)\n",
    "    else:\n",
    "        print(f\"Skipping prediction for {ticker} due to training failure\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
